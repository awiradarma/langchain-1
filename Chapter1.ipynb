{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba9a9b8-5108-43e6-a84b-b30c70c0f140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Japan?\n",
      "Answer: Let me tell you something cool!\n",
      "\n",
      "The capital of Japan is Tokyo! Can you say that with me? \"Tok-yo\". It's a very special city in Japan, and many people live there.\n",
      "\n",
      "Imagine you're on a big plane, flying all around the world, and when you finally land, what do you think you'll see? That's right! You'll see skyscrapers and lots of excitement!\n",
      "\n",
      "But Tokyo is more than just buildings. It's also where lots of yummy food, fun games, and exciting festivals happen!\n",
      "\n",
      "So, now you know that Tokyo is the capital of Japan. Isn't that awesome?\n",
      "\n",
      "Question: Tell me a short fun fact about space.\n",
      "Answer: Oh boy, do I have a cool one for you!\n",
      "\n",
      "Did you know that there is a giant storm in space called the \"Great Red Spot\" on a planet called Jupiter? It's been going around for like 300 years and it's so big that three Earths could fit inside it! Can you imagine having a storm that lasts for centuries?\n",
      "\n",
      "Isn't that OUT OF THIS WORLD (hehe, get it?)?!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Instantiate the ChatOllama model\n",
    "#    Specify the model you pulled with Ollama (e.g., \"llama3\", \"llama2\", \"mistral\")\n",
    "llm = ChatOllama(model=\"llama3.2:3b\") \n",
    "\n",
    "# 2. Define a prompt template\n",
    "#    This helps structure your input for the LLM\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Answer the user's questions like a teacher explaining to a 7 years old kid.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 3. Create a chain\n",
    "#    This pipes the prompt through the LLM and then parses the output\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 4. Invoke the chain with your question\n",
    "question = \"What is the capital of Japan?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")\n",
    "\n",
    "# Example with a different question\n",
    "question_2 = \"Tell me a short fun fact about space.\"\n",
    "response_2 = chain.invoke({\"question\": question_2})\n",
    "\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "print(f\"Answer: {response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbf3f6a-78d4-4b24-b845-fd904cc7f2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='!!! Paris', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-06T22:02:50.411456348Z', 'done': True, 'done_reason': 'stop', 'total_duration': 34997076, 'load_duration': 9068171, 'prompt_eval_count': 49, 'prompt_eval_duration': 2000000, 'eval_count': 3, 'eval_duration': 22000000, 'model_name': 'llama3.2:3b'}, id='run--aa9c21ec-bced-4f95-bf4e-75d00c80ae97-0', usage_metadata={'input_tokens': 49, 'output_tokens': 3, 'total_tokens': 52})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "system_msg = SystemMessage(\n",
    "    '''You are a helpful assistant that responds to questions with three \n",
    "        exclamation marks.''')\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "llm.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56180710-63ec-4c9b-bf97-61913e88d8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI and Cohere.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-06T22:02:56.261976442Z', 'done': True, 'done_reason': 'stop', 'total_duration': 74510400, 'load_duration': 9123753, 'prompt_eval_count': 165, 'prompt_eval_duration': 4000000, 'eval_count': 7, 'eval_duration': 60000000, 'model_name': 'llama3.2:3b'}, id='run--753a366d-f767-4836-b58e-2e0b1e6834e1-0', usage_metadata={'input_tokens': 165, 'output_tokens': 7, 'total_tokens': 172})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the\n",
    "    context below. If the question cannot be answered using the information \n",
    "    provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00cbe65-39cc-457e-951a-2976dedf8872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Two model providers are mentioned that offer Large Language Models (LLMs):\\n\\n1. Hugging Face\\n2. OpenAI\\n3. Cohere', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-06T22:03:00.656545616Z', 'done': True, 'done_reason': 'stop', 'total_duration': 229496885, 'load_duration': 9259452, 'prompt_eval_count': 163, 'prompt_eval_duration': 6000000, 'eval_count': 30, 'eval_duration': 212000000, 'model_name': 'llama3.2:3b'}, id='run--81420d4c-1f15-40f7-867b-a558ce5773f1-0', usage_metadata={'input_tokens': 163, 'output_tokens': 30, 'total_tokens': 193})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Answer the question based on the context below. If the \n",
    "        question cannot be answered using the information provided, answer with \n",
    "        \"I don\\'t know\".'''),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "    ])\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b675b6-af39-4998-857f-d729108fab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":\"They weigh the same\",\"justification\":\"A pound is a unit of weight or mass, and it's a measure of how heavy something is. The difference in density between bricks and feathers doesn't change the fact that they both weigh the same amount, one pound.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the \n",
    "        answer.'''\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''Justification for the answer'''\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "x = structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound \n",
    "    of feathers\"\"\")\n",
    "\n",
    "print (x.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1cddb3-2105-4654-9ebf-0ef446f776a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "items = parser.invoke(\"apple, banana, cherry\")\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9c8b85-f456-4e4f-ac59-6e6a9eac1eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the zebra refuse to play poker?' punchline='Because he always got striped of his money!'\n",
      "setup='Why did the cat join a band?' punchline='Because it wanted to be the purr-cussionist!'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import List\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a Pydantic model\n",
    "class Joke(BaseModel):\n",
    "    model_config = ConfigDict(\n",
    "        populate_by_name=True, # Still good to have this\n",
    "        extra='allow',\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "\n",
    "# Create a parser\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a joke about {topic} and format the output as {format_instructions}. The joke MUST be formatted as a complete and valid JSON object. Ensure there is a closing curly brace.\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature = 0) # Set temperature to 0 for deterministic output\n",
    "\n",
    "from pydantic import ValidationError\n",
    "try:\n",
    "    raw_llm_output = (prompt | llm).invoke({\"topic\": \"zebra\"}).content\n",
    "    # print (raw_llm_output)\n",
    "    joke_instance = Joke.model_validate_json(raw_llm_output) # or Joke.parse_raw(raw_llm_output) depending on Pydantic version\n",
    "    print(joke_instance)\n",
    "except ValidationError as e:\n",
    "    print(\"Pydantic Validation Error:\")\n",
    "    print(e.errors()) # This will give detailed validation errors\n",
    "except Exception as e:\n",
    "    print(f\"Other error: {e}\")\n",
    "    \n",
    "# Create an LLM chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bcb227c-e0e6-418f-9fad-19d5b05c1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\n",
      "[AIMessage(content='Oh boy, are you ready for a cool secret?\\n\\nDo you know what happens when sunlight comes into our world? It\\'s like a big hug from the sun!\\n\\nWhen the sunlight touches the Earth, it meets something called \"air\". And guess what? The air is made up of tiny, tiny particles that we can\\'t see.\\n\\nNow, here\\'s the magic part: when the sunlight hits these tiny particles in the air, it makes them do something cool. It scatters all over the sky!\\n\\nBut why does it look blue? Well, our eyes are like super powerful cameras, and they help us see everything around us. When we look up at the sky, we\\'re seeing all those tiny particles from before, bouncing off in every direction.\\n\\nAnd guess what? The color of sunlight is actually white! But when it scatters all over the sky, some of that light gets broken down into different colors. And blue is one of them!\\n\\nSo, when we look up at the sky, we\\'re seeing mostly blue because of how the tiny particles in the air are bouncing off the sunlight and scattering it all around us.\\n\\nIsn\\'t that AMAZING?', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-06T22:09:19.391434233Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1795041512, 'load_duration': 9034861, 'prompt_eval_count': 38, 'prompt_eval_duration': 3000000, 'eval_count': 234, 'eval_duration': 1781000000, 'model_name': 'llama3.2:3b'}, id='run--eb731c1e-72f3-4b9f-a333-43017712cde3-0', usage_metadata={'input_tokens': 38, 'output_tokens': 234, 'total_tokens': 272}), AIMessage(content='Let me tell you about RAINBOWS!\\n\\nSo, you know how sometimes it rains outside and the sun comes out again? That\\'s when something super cool happens - a RAINBOW appears in the sky!\\n\\nHere\\'s what makes a rainbow:\\n\\n1. **Rain drops**: When it rains, tiny drops of water fall from the sky.\\n2. **Sunlight**: The sun shines down on those rain drops.\\n3. **Reflection**: As the sunlight hits each rain drop, it bounces back inside the drop.\\n4. **Color**: Inside the rain drop, there\\'s a special thing called \"water\" that makes all kinds of colors - like red, orange, yellow, green, blue, and purple!\\n5. **Spread out**: When we look at the rainbow from far away, those tiny rain drops are spread out in front of us. That means each one is shining its colorful light into our eyes!\\n\\nBecause of something called \"refraction\" (say that with me... re-frae-shun!), the light bends a little bit as it goes through the air and reaches our eyes. This bending makes all those colors line up in the sky, creating a beautiful RAINBOW!\\n\\nSo, when you see a rainbow, just remember: rain drops, sunlight, reflection, color, spread out, and refraction all come together to make that magical, colorful sight in the sky!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-06T22:09:19.771289187Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2173934511, 'load_duration': 8438387, 'prompt_eval_count': 36, 'prompt_eval_duration': 25000000, 'eval_count': 287, 'eval_duration': 2139000000, 'model_name': 'llama3.2:3b'}, id='run--a8b3fdaa-7cdd-475b-9dfa-bf9e4e560c50-0', usage_metadata={'input_tokens': 36, 'output_tokens': 287, 'total_tokens': 323})]\n",
      "content='Good' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content='bye' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' It' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' was' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' nice' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' chatting' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' with' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' Have' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' a' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' great' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content=' day' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--56f270d5-4788-4132-ae56-05654b53c74d'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-06T22:09:19.895809365Z', 'done': True, 'done_reason': 'stop', 'total_duration': 122580771, 'load_duration': 9338622, 'prompt_eval_count': 28, 'prompt_eval_duration': 1000000, 'eval_count': 16, 'eval_duration': 110000000, 'model_name': 'llama3.2:3b'} id='run--56f270d5-4788-4132-ae56-05654b53c74d' usage_metadata={'input_tokens': 28, 'output_tokens': 16, 'total_tokens': 44}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "completion = model.invoke('Hi there!') \n",
    "print(completion.content)\n",
    "# Hi!\n",
    "\n",
    "completions = model.batch(['Explain why is the sky blue to a 5 years old', 'Explain how a rainbow formed like I am 5'])\n",
    "print(completions)\n",
    "# ['Hi!', 'See you!']\n",
    "\n",
    "for token in model.stream('Bye!'):\n",
    "    print(token)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2130d-bdfb-4917-a8a7-d8ef4ab090a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
